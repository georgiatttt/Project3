# -*- coding: utf-8 -*-
"""Project3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xT1yX30il0jRsm377koSMQmfyY77ZG5U

PLANNING
Sources: data source (hinge json files), Hinge website

Compare who you match with vs. who actually lives in your area.

Search for insta handle
Block

STEP 1. Extract:
"""

# stdlib
import json
import os
import re
import time
from collections import Counter
from concurrent.futures import ThreadPoolExecutor, as_completed
from threading import Lock

# third-party
import pandas as pd
import requests

# Replace with your real key from the assignment
DEEPSEEK_API_KEY = "sk-a7f42564324a433b836f39b479e4dfa8"

#Code Robustness!!

# --- File I/O (minimal) ---
def safe_read_json(path, default=None):
    """Read JSON at path; on error return default and note it."""
    if default is None: default = {}
    try:
        with open(path, "r", encoding="utf-8") as f:
            return json.load(f)
    except Exception as e:
        print(f"(note) failed to read JSON {path}: {e} -> using default")
        return default

def safe_write_csv(df: pd.DataFrame, path: str):
    """Write DataFrame to CSV; warn if the write fails."""
    try:
        df.to_csv(path, index=False)
    except Exception as e:
        print(f"(warning) failed to write CSV {path}: {e}")


def safe_read_csv(path, **kwargs) -> pd.DataFrame:
    """Read CSV into a DataFrame; on error return an empty DataFrame."""
    try:
        return pd.read_csv(path, **kwargs)
    except Exception as e:
        print(f"(note) failed to read CSV {path}: {e} -> returning empty DataFrame")
        return pd.DataFrame()


# --- API (minimal) ---
def http_get_json(url, *, params=None, headers=None, timeout=30):
    """Single-attempt GET with timeout + graceful fallback."""
    try:
        r = requests.get(url, params=params, headers=headers, timeout=timeout)
        r.raise_for_status()
        return r.json()
    except Exception as e:
        print(f"(note) GET {url} failed: {e} -> returning None")
        return None

def http_post_json(url, *, payload=None, headers=None, timeout=60):
    """Single-attempt POST with timeout + graceful fallback."""
    try:
        r = requests.post(url, json=payload, headers=headers, timeout=timeout)
        r.raise_for_status()
        return r.json()
    except Exception as e:
        print(f"(note) POST {url} failed: {e} -> returning None")
        return None

# Step 2: Load JSON
os.makedirs("data/raw", exist_ok=True)
data = safe_read_json("data/raw/matches.json", default=[])
if not isinstance(data, list):
    data = [data]


# Step 3: Flatten into ONE list
all_events = []

for i, match in enumerate(data):
    match_id = i

    # --- Match event ---
    if "match" in match:
        for m in match["match"]:
            all_events.append({
                "match_id": match_id,
                "event_type": "match",
                "timestamp": m.get("timestamp"),
                "content": None,
                "extra_info": None
            })

    # --- Chat messages ---
    if "chats" in match:
        for c in match["chats"]:
            all_events.append({
                "match_id": match_id,
                "event_type": "chat",
                "timestamp": c.get("timestamp"),
                "content": c.get("body"),
                "extra_info": None
            })

    # --- Likes ---
    if "like" in match:
        for l in match["like"]:
            comment = l.get("like", [{}])[0].get("comment") if l.get("like") else None
            all_events.append({
                "match_id": match_id,
                "event_type": "like",
                "timestamp": l.get("timestamp"),
                "content": None,
                "extra_info": comment
            })

    # --- Blocks ---
    if "block" in match:
        for b in match["block"]:
            all_events.append({
                "match_id": match_id,
                "event_type": "block",
                "timestamp": b.get("timestamp"),
                "content": None,
                "extra_info": b.get("block_type")
            })

    # --- We Met feedback ---
    if "we_met" in match:
        for w in match["we_met"]:
            all_events.append({
                "match_id": match_id,
                "event_type": "we_met",
                "timestamp": w.get("timestamp"),
                "content": None,
                "extra_info": w.get("choice")
            })

# Step 4: Convert to DataFrame
df_all = pd.DataFrame(all_events)

# Step 5: Save & download single CSV
safe_write_csv(df_all, "hinge_all_events.csv")
#files.download("hinge_all_events.csv")

print("✅ Exported single CSV: hinge_all_events.csv")

df_all.head()

"""DATA CLEANING: I want to organize the events by time so the chats make sense."""

# 1) Parse timestamps safely
df_all['timestamp'] = pd.to_datetime(df_all['timestamp'], errors='coerce', utc=True)

# 2) (Optional) stable tie-break so simultaneous events are ordered nicely
order = pd.CategoricalDtype(
    categories=['match','chat','like','we_met','block'],  # tweak if you prefer
    ordered=True
)
df_all['event_type'] = df_all['event_type'].astype(order)

# 3) Sort within each match, then by timestamp, then by event_type
df_all_sorted = df_all.sort_values(['match_id','timestamp','event_type']).reset_index(drop=True)

# 4) Save / replace
safe_write_csv(df_all_sorted, "hinge_all_events_sorted.csv")

df_all_sorted.head()

"""Next: connect w API and deepseek"""

# ---- constants ----
STOPWORDS = {
    "a", "an", "the", "and", "or", "but", "if", "so", "to", "of", "in", "on",
    "for", "with", "at", "by", "from", "as", "is", "am", "are", "was", "were",
    "be", "been", "being", "do", "does", "did", "have", "has", "had", "not",
    "no", "yes", "yeah", "yep", "ok", "okay", "i", "me", "my", "mine", "you",
    "your", "yours", "he", "she", "it", "we", "they", "them", "their", "our",
    "this", "that", "these", "those", "there", "here", "what", "which", "who",
    "whom", "when", "where", "why", "how",
}



_cache_lock = Lock()

def dict_lookup(word: str):
    # Treat stopwords as known
    if word in STOPWORDS:
        return {"found": True, "data": None}

    with _cache_lock:
        if word in _dict_cache:
            return _dict_cache[word]

    url = f"https://api.dictionaryapi.dev/api/v2/entries/en/{word}"
    result = {"found": False, "data": None}
    for attempt in range(3):
        try:
            r = requests.get(url, timeout=8)
            if r.status_code == 200:
                result = {"found": True, "data": r.json()}
                break
            if r.status_code in (429, 500, 502, 503, 504):
                time.sleep(0.4 * (attempt + 1))
                continue
            break
        except Exception:
            time.sleep(0.4 * (attempt + 1))

    # Save result into cache
    with _cache_lock:
        _dict_cache[word] = result
    return result
'''
print("Looking up dictionary entries (threaded)…")
not_cached = [w for w in top_words if w not in _dict_cache]

with ThreadPoolExecutor(max_workers=8) as ex:
    futures = [ex.submit(dict_lookup, w) for w in not_cached]
    for i, fut in enumerate(as_completed(futures), 1):
        _ = fut.result()
        if i % 100 == 0:
            # take a snapshot copy under lock, then write to disk
            with _cache_lock:
                snapshot = dict(_dict_cache)
            _write_json(DICT_CACHE_FN, snapshot)

# final write after all lookups
with _cache_lock:
    snapshot = dict(_dict_cache)
_write_json(DICT_CACHE_FN, snapshot)

print(f"Dictionary cache size: {len(snapshot)}")
'''

# ---------- config ----------
OUT_PATH     = "hinge_style_enrichment.csv"
SAMPLE_LEN   = 1200     # truncate convo text sent to DeepSeek
MIN_CHARS    = 0       # don't skip extremely short convos
FLUSH_EVERY  = 20       # checkpoint frequency
THROTTLE_SEC = 0.35     # be nice to the API

# ---------- build per-conversation text blocks ----------
df = safe_read_csv("hinge_all_events_sorted.csv")
df_chat = df[df["event_type"] == "chat"].copy()
df_chat = df_chat.sort_values(["match_id", "timestamp"])

agg = (
    df_chat.groupby("match_id")["content"]
           .apply(lambda s: "\n".join(str(x) for x in s if pd.notna(x)))
           .reset_index(name="chat_text")
)

# (optional) skip tiny convos
agg["len"] = agg["chat_text"].str.len().fillna(0).astype(int)
agg = agg[agg["len"] >= MIN_CHARS].drop(columns="len").reset_index(drop=True)

# ---------- resume if output exists ----------
done_df = safe_read_csv(OUT_PATH)
done_ids = set()
if not done_df.empty:
    done_ids = set(done_df["match_id"].astype(str))
    print(f"✔ Resume mode: found {len(done_ids)} completed rows in {OUT_PATH}")


# ---------- helpers ----------
def _safe_enrich(match_id: str, text: str):
    """Wraps dictionary + DeepSeek so errors don't stop the loop."""
    try:
        feats = build_dict_features(text)
    except Exception as e:
        feats = {
            "word_count": None, "unique_words": None, "pct_standard_words": None,
            "slang_ratio": None, "avg_defs_per_known_word": None,
            "pos_share_noun": None, "pos_share_verb": None,
            "pos_share_adj": None, "pos_share_adv": None,
            "synonym_hit_rate": None, "top_unknown_words": []
        }
        # include the error in suggestions later
        insights = {
            "style_summary": None, "formality": None, "sentiment": None,
            "topics": None, "strengths": None,
            "suggestions": [f"dict features error: {e}"],
            "estimated_readability_grade": None
        }
        return {"match_id": match_id, **feats, **insights}

    try:
        insights = deepseek_analyze(feats, (text or "")[:SAMPLE_LEN])
    except Exception as e:
        insights = {
            "style_summary": None, "formality": None, "sentiment": None,
            "topics": None, "strengths": None,
            "suggestions": [f"DeepSeek error: {e}"],
            "estimated_readability_grade": None
        }
    return {"match_id": match_id, **feats, **insights}

# ---------- main loop with checkpoints & ETA ----------
rows = [] if done_df.empty else done_df.to_dict("records")

to_process = [(str(r.match_id), r.chat_text) for r in agg.itertuples(index=False)
              if str(r.match_id) not in done_ids]

total = len(to_process)
print(f"Will process {total} conversations…")

start = time.time()
for i, (mid, text) in enumerate(to_process, 1):
    t0 = time.time()
    record = _safe_enrich(mid, text)
    rows.append(record)
    done_ids.add(mid)

    # checkpoint every FLUSH_EVERY
    if i % FLUSH_EVERY == 0 or i == total:
        safe_write_csv(pd.DataFrame(rows), OUT_PATH)
        # progress + ETA
        elapsed = time.time() - start
        avg = elapsed / i
        remaining = total - i
        eta_min = (remaining * avg) / 60
        took = time.time() - t0
        print(
    f"✓ {i}/{total} saved • last {took:.1f}s • "
    f"ETA ~{eta_min:.1f} min"
)

    time.sleep(THROTTLE_SEC)  # gentle rate limit

print(f"✅ Done. Wrote {len(rows)} rows to {OUT_PATH}")

# cache init (needed even if prefetch is commented out)
DICT_CACHE_FN = "dict_cache.json"
_cache_lock = Lock()
_dict_cache = safe_read_json(DICT_CACHE_FN, default={})

# --- Recompute dictionary features only, then merge into the enriched CSV ---
# Load existing outputs and events
df_enriched = safe_read_csv("hinge_style_enrichment.csv")
df_events   = safe_read_csv("hinge_all_events_sorted.csv")


# Build per-conversation text
df_chat = df_events[df_events["event_type"]=="chat"].copy()
df_chat = df_chat.sort_values(["match_id","timestamp"])
agg = (
    df_chat.groupby("match_id")["content"]
           .apply(lambda s: "\n".join(str(x) for x in s if pd.notna(x)))
           .reset_index(name="chat_text")
)

# Tokenizer
WORD_RE = re.compile(r"[a-zA-Z']+")
def tokenize(text: str):
    """Tokenize text to lowercase words (letters and apostrophes)."""
    return [w.lower() for w in WORD_RE.findall(text or "")]

# Helper to summarize dictionary entries (same logic you used)
def summarize_dict_entries(entries):
    """Summarize entries: def count, POS set, synonym flag."""
    defs = 0; pos=set(); has_syn=False
    for entry in entries or []:
        for m in entry.get("meanings", []):
            if m.get("partOfSpeech"):
                pos.add(m["partOfSpeech"].lower())
            defs += len(m.get("definitions", []))
            if m.get("synonyms"):
                has_syn = True
    return defs, pos, has_syn

# Recompute features with stopwords counted as KNOWN (no DeepSeek calls)
recalc_rows = []
for r in agg.itertuples(index=False):
    mid = r.match_id
    text = r.chat_text or ""
    toks = tokenize(text)
    vc = Counter(toks)

    unique_words = len(vc)
    if unique_words == 0:
        recalc_rows.append({
            "match_id": mid, "word_count": 0, "unique_words": 0,
            "pct_standard_words": None, "slang_ratio": None,
            "avg_defs_per_known_word": None,
            "pos_share_noun": None, "pos_share_verb": None,
            "pos_share_adj": None, "pos_share_adv": None,
            "synonym_hit_rate": None, "top_unknown_words": []
        })
        continue

    found_cnt=0; defs_total=0; syn_hits=0
    pos_counter={"noun":0,"verb":0,"adjective":0,"adverb":0}
    unknown=[]

    for w, freq in vc.items():
        # treat stopwords as known up-front
        if w in STOPWORDS:
            found_cnt += 1
            continue

        info = _dict_cache.get(w)
        if info is None:
            # fall back to your dict_lookup (uses cache + retry)
            info = dict_lookup(w)

        if info.get("found"):
            defs, pos, has_syn = summarize_dict_entries(info.get("data"))
            found_cnt += 1
            defs_total += defs
            if has_syn: syn_hits += 1
            for p in pos:
                if p.startswith("noun"): pos_counter["noun"] += 1
                elif p.startswith("verb"): pos_counter["verb"] += 1
                elif p.startswith("adject"): pos_counter["adjective"] += 1
                elif p.startswith("adverb"): pos_counter["adverb"] += 1
        else:
            if len(w) > 1:
                unknown.append((w, int(freq)))

    pct_standard = found_cnt / unique_words if unique_words else None
    slang_ratio  = (1 - pct_standard) if pct_standard is not None else None
    avg_defs     = (defs_total / found_cnt) if found_cnt else None
    syn_rate     = (syn_hits / found_cnt) if found_cnt else None
    pos_share    = {k:(v/unique_words) for k,v in pos_counter.items()}

    unknown.sort(key=lambda x:(-x[1], x[0]))
    top_unknown = [w for w,_ in unknown[:10]]

    recalc_rows.append({
        "match_id": mid,
        "word_count": int(sum(vc.values())),
        "unique_words": int(unique_words),
        "pct_standard_words": pct_standard,
        "slang_ratio": slang_ratio,
        "avg_defs_per_known_word": avg_defs,
        "pos_share_noun": pos_share["noun"],
        "pos_share_verb": pos_share["verb"],
        "pos_share_adj":  pos_share["adjective"],
        "pos_share_adv":  pos_share["adverb"],
        "synonym_hit_rate": syn_rate,
        "top_unknown_words": top_unknown
    })

df_fix = pd.DataFrame(recalc_rows)

# Merge: replace ONLY the dictionary columns, keep DeepSeek columns as-is
dict_cols = [
    "word_count","unique_words","pct_standard_words","slang_ratio",
    "avg_defs_per_known_word","pos_share_noun","pos_share_verb",
    "pos_share_adj","pos_share_adv","synonym_hit_rate","top_unknown_words"
]
df_v2 = df_enriched.drop(columns=[c for c in dict_cols if c in df_enriched.columns]) \
                   .merge(df_fix[["match_id"] + dict_cols], on="match_id", how="left")

safe_write_csv(df_v2, "hinge_style_enrichment_v2.csv")
print("✅ wrote hinge_style_enrichment_v2.csv")

# Quick sanity check: none of these should appear in top_unknown_words anymore
sus = {"and","for","what","is","to","of","in"}
hits = df_v2["top_unknown_words"].astype(str).str.contains("|".join(sus)).sum()
print(f"suspected stopwords still flagged as unknown: {hits}")
df_v2.head(3)

"""NOTE: hinge_style_enrichment.csv is the updated version"""

df_enriched.head()

"""EXPLAINATION OF COLS

word_count: Total number of words in the messages for a given match.

unique_words: Number of unique words used.

pct_standard_words: Fraction of words that the dictionary recognized. High = mostly standard English.

slang_ratio: Fraction of words not in the dictionary (slang, acronyms, typos, emojis). Calculated (1 − pct_standard_words).

avg_defs_per_known_word: Average number of definitions per recognized word (proxy for vocabulary richness).

pos_share_noun / pos_share_verb / pos_share_adj / pos_share_adv: Proportion of your unique words that fall into each part of speech. E.g., high adjectives = descriptive style.

synonym_hit_rate: Proportion of recognized words that had synonyms listed.

top_unknown_words: A list of your most frequent words the dictionary couldn’t find (slang/shortcuts like “lol,” “tbh,” etc.).

NEXT: visualizations
"""

df_enriched["estimated_readability_grade"].value_counts()

"""notes on the process:

step 1. I downloaded my info from Hinge

step 2. clean hinge info. I changed JSON to CSV and ordered the rows by time (so the chats made more sense). For reference, the chats only include my side of each conversation.

step 3. connect w API (dictionary) and deepseek. This part took a lot of tries. First, I had issues with runtime because I was calling deepseek and the dictionary for every single coversation. It was taking forever to load, so I changed it so that they were called less frequently (every week). Second, the dictionary API does not contain definitions for words like and/for/to/etc. My program retrieved definitions of words in order to determine if a word was standard english or not. The idea was to identify slang or abbreviations. However, since transitional words were not included in the dictionary, they were being included in 'top_unknown_words', where I was trying to isolate slang. Hardcoding it did not solve the issue, but since the block had taken so long to run, I decided to fix the issue in processing instead of during the API calls.

potential issues:
Code Robustness
• Includes error handling for API calls and file I/O, and is resilient
Style
• Code adheres to the PEP 8 style guide

all the documentation and structure
"""